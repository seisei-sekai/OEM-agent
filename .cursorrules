# Cursor Rules 

## Rule 1: Markdown Files Management

**NEVER create markdown files in the root directory.**

- All markdown documentation files MUST be placed in the `docs/` folder
- When creating a new markdown file, include a header with:
  - Creation date
  - Last updated date
  - Purpose/description
- Format:
  ```markdown
  # Document Title
  
  **Created:** YYYY-MM-DD-MM-SS (Tokyo TIme)
  **Last Updated:** YYYY-MM-DD-MM-SS (Tokyo Time)
  **Purpose:** Brief description
  
  ---
  
  [Content]
  ```
- Update `docs/INDEX.md` when creating new documentation files
- Only create markdown files when explicitly requested or necessary for documentation

## Rule 2: Domain Driven Design (DDD)

**Always follow Domain Driven Design principles when writing code.**

- Organize code by business domains (bounded contexts)
- Use domain entities, value objects, and aggregates
- Separate domain logic from infrastructure and presentation layers
- Structure:
  - `domain/` - Business logic and domain models
  - `application/` - Use cases and application services
  - `infrastructure/` - External integrations and data access
  - `api/` or `presentation/` - API routes and UI components

## Rule 3: Keep Rules Concise

**Keep cursor rules brief and focused.**

- Avoid verbose explanations
- Focus on essential guidelines only
- Let code patterns emerge from practice

## Rule 4: All the Annotation should be in English

**All the Annotation should be in English**

- If there is Adding Chinese or Japanese Annotation, Please also add a english version in the following place.


## Rule 5: Operate by Best Practices

**All operations must follow best practices.**

- Do not automatically execute or implement any operation if it does not adhere to best practices.
- Do not document non-best-practice operations in any markdown file.
- Evaluate actions for adherence to current best practices before proceeding.

## Rule 6: Context Relevance Filtering for AI Agent

**If the context is too large, prioritize and filter context dynamically.**

- The AI Agent must evaluate which parts of the context are essential and which are non-essential.
- Irrelevant or unnecessary context should be excluded from processing.
- Always use dynamic and intelligent strategies to determine relevance based on the current task.


## Rule 7: Best Practice - Explain Before You Code

**Before writing code, first explain your approach.**

- Always provide a brief explanation of your thought process and approach before beginning to write code.
- Ensure the explanation is relevant and concise, focusing on how you intend to solve the problem or implement the feature.



## Rule 8: Senior Developer Persona

- You are an expert senior software engineer.
- Do not explain basic concepts.
- Focus on high-level architecture, scalability, and best practices.

## Rule 9: No "Yapping"

- No conversational filler or unnecessary commentary.
- Deliver only direct, relevant content.

## Rule 10: Be Concise

- Output only the necessary information.
- Avoid redundant verbosity.
- No preambles.

## Rule 11: Chain of Thought

- Before coding, briefly outline steps in a <thinking> tag.
- Ensure logic is clear and sequential.

## Rule 12: Admit Uncertainty

- If uncertain about version, API, or syntax, declare it explicitly.
- Never guess or hallucinate technical details.


# Project-Scope Specific Rules

## Turborepo Best Practices

- Use workspace protocol (`workspace:*`) for internal package dependencies
- Place shared code in `/packages` (e.g., `@repo/ui`, `@repo/tsconfig`)
- Define tasks in root `turbo.json` with proper dependency graph
- Leverage remote caching for CI/CD pipelines
- Keep each workspace focused with single responsibility
- Use `turbo run build --filter` for selective builds

## Next.js Best Practices

- Use App Router (app directory) over Pages Router for new features
- Implement Server Components by default; use Client Components (`'use client'`) only when necessary
- Use `next/image` for automatic image optimization
- Implement proper loading.tsx and error.tsx for better UX
- Use Route Handlers (app/api) over API routes
- Leverage streaming with Suspense boundaries
- Implement proper metadata exports for SEO
- Use `generateStaticParams` for dynamic routes when possible

## Hono Best Practices

- Structure routes using method chaining for clarity
- Use middleware for cross-cutting concerns (auth, logging, CORS)
- Leverage Hono's context type safety with generics
- Keep route handlers thin; delegate to service layer
- Use `zValidator` middleware for request validation
- Return proper HTTP status codes and error responses
- Use environment variables via `c.env` for runtime config

## LangGraph.js (TypeScript) Best Practices

- Define clear state schemas with Zod or TypeScript types
- Keep nodes pure and testable; avoid side effects where possible
- Use conditional edges for dynamic flow control
- Implement proper error handling at node level
- Structure graphs as reusable compositions
- Use checkpointing for long-running workflows
- Document graph topology with clear node purposes
- Type all state transitions explicitly

## Terraform Best Practices

- Structure by environment (dev, staging, prod) with modules
- Use remote state (GCS backend for GCP)
- Implement proper variable validation and descriptions
- Use `terraform fmt` and `terraform validate` in CI
- Never commit `.tfstate` files or sensitive variables
- Use data sources over hardcoded values
- Implement resource tagging/labeling for cost tracking
- Use workspaces for environment isolation


## Docker Best Practices

- Use multi-stage builds to reduce image size
- Implement `.dockerignore` to exclude unnecessary files
- Use specific base image tags (avoid `latest`)
- Run containers as non-root user
- Use build arguments for configurable builds
- Leverage layer caching; order Dockerfile instructions by change frequency
- Use health checks (HEALTHCHECK instruction)
- Scan images for vulnerabilities in CI

## Docker + TypeScript Compilation ⚠️ CRITICAL

**NEVER mask compilation errors with `|| echo` in Dockerfile.**

**Problem**: TypeScript incremental compilation (`.tsbuildinfo`) can cause partial files to not recompile, leading to stale code in Docker containers even after source changes.

**Symptoms**:
- Code changes not reflected in running containers
- Missing logs or functions that were recently added
- Docker build succeeds but uses old compiled JS files

**Solution**:
```dockerfile
# ❌ BAD - Masks compilation errors
RUN pnpm build || echo "Build completed with warnings"

# ✅ GOOD - Fails fast on errors
RUN pnpm build
```

**When code doesn't update in Docker**:
1. Stop containers: `docker-compose down`
2. Delete local build artifacts: `rm -rf packages/*/dist packages/*/*.tsbuildinfo`
3. Rebuild without cache: `docker-compose build --no-cache`
4. Verify compiled code in container: `docker exec <container> grep "new code" /app/path/to/file.js`

**Prevention**:
- Always check Docker container contents after deploy if behavior seems wrong
- Use `--no-cache` flag when debugging compilation issues
- Remove error suppression from Dockerfiles (`|| echo`, `|| true`, etc.)

### TypeScript Build Cache (.tsbuildinfo) ⚠️ CRITICAL

**NEVER commit `.tsbuildinfo` files to Git.**

**Problem**: `.tsbuildinfo` files contain TypeScript incremental compilation cache. When committed to Git, these stale cache files can cause:
- TypeScript compiler to skip recompiling files
- Old import paths (e.g., with `.js` extensions) to persist
- "Cannot find module" errors in Docker builds even after source code is fixed
- Different behavior between local and Docker builds

**Symptoms**:
- Local build succeeds, Docker build fails with module resolution errors
- TypeScript generates `.d.ts.map` but not `.d.ts` files
- Changes to import statements not reflected in compilation
- Build "succeeds" but produces no output files

**Root Cause Example**:
```typescript
// Old code (committed in .tsbuildinfo cache):
export * from './entities/index.js';  // ❌ TypeScript can't find this

// New code (in source):
export * from './entities/index';     // ✅ Correct
```
If `.tsbuildinfo` is committed with old state, TypeScript uses cached info and skips the file.

**Solution**:
```bash
# 1. Add to .gitignore
echo "*.tsbuildinfo" >> .gitignore

# 2. Remove from Git tracking
git rm --cached **/*.tsbuildinfo
git commit -m "fix: remove TypeScript build cache from git"

# 3. Clean in Dockerfile BEFORE build
RUN find . -name "*.tsbuildinfo" -delete && pnpm build
```

**Required .gitignore entries**:
```gitignore
# TypeScript
*.tsbuildinfo

# Build outputs
dist/
build/
.next/
```

**Debugging checklist**:
1. Check if `.tsbuildinfo` exists: `find . -name "*.tsbuildinfo" | grep -v node_modules`
2. Verify it's in `.gitignore`: `git check-ignore *.tsbuildinfo`
3. Confirm not tracked: `git ls-files | grep tsbuildinfo` (should be empty)
4. Clean locally: `find . -name "*.tsbuildinfo" -delete`
5. Clean in Docker: Add delete step before build in Dockerfile

**For composite projects (project references)**:
- Use `tsc --build` instead of `tsc`
- Ensure all package.json have `"build": "tsc --build"`
- Composite projects REQUIRE `.tsbuildinfo` for incremental builds, but these must be generated per build, never committed

## CORS Configuration for Cloud Deployment ⚠️ CRITICAL

**NEVER hardcode localhost-only origins in CORS middleware.**

**Problem**: CORS middleware configured only for `localhost:3000` blocks access from deployed instances using IP addresses or custom domains.

**Symptoms**:
- Browser console shows: "Access-Control-Allow-Origin header has a value 'http://localhost:3000' that is not equal to the supplied origin"
- API calls fail with `net::ERR_FAILED`
- Frontend deployed on cloud (e.g., GCP) cannot communicate with API

**Root Cause Example**:
```typescript
// ❌ BAD - Only allows localhost
export const corsMiddleware = cors({
  origin: 'http://localhost:3000',
  credentials: true,
});

// ❌ ALSO BAD - Hardcoded list without flexibility
export const corsMiddleware = cors({
  origin: (origin) => {
    const allowed = ['http://localhost:3000'];
    return allowed.includes(origin) ? origin : allowed[0];
  },
});
```

**Solution**:
```typescript
// ✅ GOOD - Flexible for development and production
export const corsMiddleware = cors({
  origin: (origin) => {
    const allowedOrigins = [
      'http://localhost:3000',
      'http://localhost:3001',
      process.env.FRONTEND_URL || '',
    ].filter(Boolean);

    // Allow if origin is in whitelist
    if (allowedOrigins.includes(origin)) {
      return origin;
    }
    
    // Allow if no origin (same-origin)
    if (!origin) {
      return origin;
    }
    
    // For production: allow IP-based origins (e.g., http://34.84.2.46:3000)
    // This handles direct IP access to deployed apps
    if (origin.match(/^https?:\/\/\d+\.\d+\.\d+\.\d+:\d+$/)) {
      return origin;
    }
    
    // Fallback to first allowed origin
    return allowedOrigins[0];
  },
  credentials: true,
  allowMethods: ['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS'],
  allowHeaders: ['Content-Type', 'Authorization'],
});
```

**Environment Variables**:
```bash
# Local .env
FRONTEND_URL=http://localhost:3000

# GCP .env
FRONTEND_URL=http://34.84.2.46:3000
# Or for custom domain
FRONTEND_URL=https://yourdomain.com
```

**Testing CORS**:
```bash
# Test OPTIONS preflight
curl -v -X OPTIONS "http://localhost:4000/api/endpoint" \
  -H "Origin: http://34.84.2.46:3000" \
  -H "Access-Control-Request-Method: GET"

# Should return:
# < access-control-allow-origin: http://34.84.2.46:3000
```

**Deployment Checklist**:
1. Set `FRONTEND_URL` in production `.env`
2. Verify CORS allows IP-based origins if needed
3. Test with actual deployed frontend URL
4. Check browser console for CORS errors
5. Restart API container after `.env` changes

**Security Notes**:
- IP regex only matches IPv4 format (prevents arbitrary origins)
- Always validate environment variables exist
- Use HTTPS in production for both frontend and API
- Consider adding rate limiting for public APIs

## OpenAI API Key Configuration ⚠️ CRITICAL

**NEVER commit API keys or use placeholder values in production.**

**Problem**: Using placeholder API keys (`sk-placeholder`) or committing real keys to Git causes runtime failures or security breaches.

**Symptoms**:
- API returns: "Incorrect API key provided: sk-place**lder"
- 401 Unauthorized errors from OpenAI API
- Application works locally but fails in production
- API key exposed in Git history

**Root Cause**:
1. **Placeholder values in production `.env`**: When deployment scripts create `.env`, they use dummy values
2. **Committed keys in Git**: Real API keys committed to repository
3. **Container not restarted**: After updating `.env`, old environment variables persist in running containers

**Solution**:

### 1. Never Commit API Keys
```gitignore
# .gitignore
.env
.env.local
.env.production
*.key
credentials.json
```

### 2. Use Environment Variables Properly
```typescript
// ✅ GOOD - Fail fast if missing
const openaiKey = process.env.OPENAI_API_KEY;
if (!openaiKey || openaiKey === 'sk-placeholder') {
  throw new Error('OPENAI_API_KEY must be set in environment');
}

// ✅ GOOD - Validate format
if (!openaiKey.startsWith('sk-proj-') && !openaiKey.startsWith('sk-')) {
  throw new Error('Invalid OPENAI_API_KEY format');
}
```

### 3. Deployment Script Best Practices
```bash
# ❌ BAD - Creates .env with placeholder
cat > .env << EOF
OPENAI_API_KEY=sk-placeholder
EOF

# ✅ GOOD - Prompt for real key or copy from local
if [ -f .env.local ]; then
  echo "Copying API key from .env.local..."
  grep "OPENAI_API_KEY" .env.local >> .env
else
  echo "ERROR: .env.local not found. Set OPENAI_API_KEY manually."
  exit 1
fi
```

### 4. GCP Deployment Workflow
```bash
# On local machine:
# 1. Ensure .env has real key
grep "OPENAI_API_KEY" .env | grep -v "placeholder" || echo "ERROR: Set real API key!"

# 2. Deploy and update key on GCP
gcloud compute ssh vm-name --command="
cd /opt/app
sudo sed -i 's/OPENAI_API_KEY=.*/OPENAI_API_KEY=sk-proj-REAL-KEY-HERE/' .env

# CRITICAL: Restart containers to load new env
sudo docker-compose down
sudo docker-compose up -d
"
```

### 5. Verification Checklist
```bash
# Check .env file (obfuscate output)
grep "OPENAI_API_KEY" .env | cut -c1-50

# Test API key works
curl https://api.openai.com/v1/models \
  -H "Authorization: Bearer $OPENAI_API_KEY" | jq '.data[0].id'

# Verify container environment
docker exec container-name env | grep OPENAI_API_KEY | cut -c1-50

# Check application logs for API errors
docker logs container-name 2>&1 | grep -i "api key\|401\|unauthorized"
```

**Container Restart Rules**:
- **Always restart after `.env` changes**: `docker-compose restart service-name`
- **Or full restart**: `docker-compose down && docker-compose up -d`
- **Never assume**: Environment variables auto-reload (they don't!)

**Security Best Practices**:
1. Use separate keys for dev/staging/prod
2. Rotate keys regularly (monthly recommended)
3. Monitor API usage for anomalies
4. Use `.env.example` with placeholder values for documentation
5. Use secret management services (GCP Secret Manager, AWS Secrets Manager) for production

**Emergency Response**:
If API key is committed to Git:
```bash
# 1. Immediately revoke the key at OpenAI dashboard
# 2. Generate new key
# 3. Update all environments
# 4. Remove from Git history (if recent)
git filter-branch --force --index-filter \
  "git rm --cached --ignore-unmatch .env" \
  --prune-empty --tag-name-filter cat -- --all

# 5. Force push (coordinate with team!)
git push origin --force --all
```

## Test-Driven Development (TDD) for AI Coding

- Write tests BEFORE implementation (red-green-refactor)
- Test not just happy paths but edge cases and error conditions
- Verify tests fail initially (prove they catch issues)
- Use integration tests to validate end-to-end flows
- Mock external dependencies (APIs, databases) in unit tests
- Implement snapshot tests for UI components
- Run tests in CI pipeline; block merges on test failures
- Test that errors are properly handled and logged
- Validate that async operations complete correctly (not just surface-level execution)

## MongoDB Best Practices

- Use validation schemas for collections to enforce data integrity
- Create appropriate indexes for high-frequency queries; avoid unnecessary indexes
- Prefer using ObjectIDs for primary keys
- Use replica sets for high availability and data redundancy
- Regularly back up production databases and test restore procedures
- Avoid unbounded schema growth (e.g., unbounded arrays or documents ≥ 16MB)
- Separate operational and analytical workloads with dedicated clusters or read replicas
- Manage database user permissions using role-based access control (RBAC)
- Monitor and tune query performance with profiling tools
- Store sensitive data encrypted at rest and in transit

## Feature Development Workflow (Scrum + TDD + DDD)

**Use automated workflow for all new features.**

### Quick Start

```bash
# 1. Start new feature
pnpm workflow:new-feature feature-name

# 2. Fill requirement: Business/Features/feature-name/requirement.md
# 3. Generate design
pnpm workflow:generate-design feature-name

# 4. Generate tests (TDD Red phase)
pnpm workflow:generate-tests feature-name

# 5. Generate code stubs
pnpm workflow:generate-stubs feature-name

# 6. Implement code (TDD Green phase)
# Make tests pass

# 7. Prepare PR (auto-generates DDD diff)
pnpm workflow:prepare-pr
```

### Workflow Rules

**Requirements:**
- Document in `Business/Features/{name}/requirement.md`
- Use provided Scrum template
- Link to Notion for full context
- Define all domain/application/infrastructure changes

**Design:**
- Auto-generated from requirements
- Review and adjust as needed
- Includes Mermaid diagrams

**Testing (TDD):**
- Tests generated BEFORE implementation
- Run `pnpm test` - should fail (Red phase)
- Implement code to make tests pass (Green phase)
- Refactor while keeping tests green
- Minimum 80% coverage required

**Commits:**
- Pre-commit hook auto-generates DDD snapshot
- Snapshot stored in `docs/ddd-snapshots/{date}-{sha}/`
- Includes all 9 DDD diagrams + metadata

**Pull Requests:**
- Use `pnpm workflow:prepare-pr` for automation
- Auto-generates DDD diff diagrams (before/after)
- Stored in `docs/ddd-changes/PR-{number}/`
- PR template auto-populated with:
  - Test results
  - DDD impact analysis
  - Review focus areas
  - Side-by-side diagram comparisons

### DDD Diagram Versioning

**Per Commit:** Full DDD snapshot in `docs/ddd-snapshots/`
**Per PR:** Side-by-side comparison in `docs/ddd-changes/`

**View changes:**
```bash
# See all snapshots
ls docs/ddd-snapshots/

# View PR diff
open docs/ddd-changes/PR-{number}/comparison.md
```

### Code Review Guidelines

**Use DDD diff diagrams:**
- Review architecture impact visually
- Check layer separation maintained
- Verify aggregate boundaries
- Ensure domain events properly used

**Focus areas (AI-identified):**
- Critical business logic changes
- Security-sensitive integrations
- Performance implications
- Breaking changes

**Checklist:**
- [ ] Domain logic in domain layer only
- [ ] Use cases are thin orchestration
- [ ] Tests written before implementation
- [ ] Coverage >80%
- [ ] DDD principles followed
- [ ] No breaking changes to public APIs

### Documentation

- **Workflow Guide:** `docs/WORKFLOW_GUIDE.md`
- **DDD Models:** `docs/ddd-models/README.md`
- **Templates:** `templates/scrum/requirement-template.md`

